{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vision.image_classification.mobilenet import load_model, MobileNetModelsType\n",
    "from weight_type import WeightType\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.transforms.transforms.Compose"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MobileNetModelsType.mobilenet_v2, WeightType.Pretrained, num_class=NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.rand((4, 3, 640, 640)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([731,  18, 791, 731])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.softmax(output, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_v3_model = load_model(MobileNetModelsType.mobilenet_v3_small, WeightType.Pretrained, num_class=NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "  (1): Hardswish()\n",
       "  (2): Dropout(p=0.2, inplace=True)\n",
       "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_v3_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_v3_model = load_model(MobileNetModelsType.mobilenet_v3_large, WeightType.Pretrained, num_class=NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_v3_model.classifier[-1].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "composed = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomResizedCrop(224, 224),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resize(always_apply=False, p=1, height=256, width=256, interpolation=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Compose' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m composed[\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mResize(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Compose' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "composed[0] = A.Resize(224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.vision.augmentations import create_transforms\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "albu_transforms = create_transforms(is_train=True, img_width=224, img_height=224, use_random_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.rand(334, 500, 3).astype(np.float32)\n",
    "res = albu_transforms(image=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZY\\Anaconda3\\envs\\pytorch_boilerplate\\Lib\\site-packages\\albumentations\\augmentations\\dropout\\cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n",
      "c:\\Users\\ZY\\Anaconda3\\envs\\pytorch_boilerplate\\Lib\\site-packages\\albumentations\\core\\composition.py:53: UserWarning: transforms is single transform, but a sequence is expected! Transform will be wrapped into list.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/10000\n",
      "2000/10000\n",
      "3000/10000\n",
      "4000/10000\n",
      "5000/10000\n",
      "6000/10000\n",
      "7000/10000\n",
      "8000/10000\n",
      "9000/10000\n"
     ]
    }
   ],
   "source": [
    "img = np.random.rand(224,224,3).astype(np.float32)\n",
    "\n",
    "for i in range(10000):\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i}/{10000}')\n",
    "    albu_transforms = create_transforms(is_train=True, img_width=224, img_height=224, use_random_crop=False)\n",
    "    res = albu_transforms(image=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_boilerplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4261957a9d8b57d7b727da34e29b6c0942a2dd968adb8306319a82a79b99c195"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
